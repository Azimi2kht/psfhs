{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4653dc2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 90152,
     "status": "ok",
     "timestamp": 1702553771205,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "0ZbOmGM8D3hr",
    "outputId": "1e802c4d-6a78-40bb-ae84-880973386fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.1 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.23.1.tar.gz (10.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m285.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.10.0 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.16.0rc0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.10.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b7b7a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 7186,
     "status": "ok",
     "timestamp": 1702553778384,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "857a5f70"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the necessary libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01malbu\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import tensorflow as tf\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net\n",
    "from ImageLoader import ImageLoader2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1263919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1702553779333,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
    "outputId": "4f7fdd6f-1255-4429-c17f-7a3c5f970052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of GPUs available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07aa714f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702553779333,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "9edbe667"
   },
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "\n",
    "img_size = 352\n",
    "dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
    "learning_rate = 1e-4\n",
    "seed_value = 58800\n",
    "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "ct = datetime.now()\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
    "progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
    "plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
    "model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
    "\n",
    "EPOCHS = 600\n",
    "min_loss_for_saving = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a271814",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 502969,
     "status": "ok",
     "timestamp": 1702554282301,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "264050d8",
    "outputId": "1b37bcdd-f2b7-454b-e972-7ee79e0e7d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [08:18,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "X, Y = ImageLoader2D.load_data(img_size, img_size, -1, 'kvasir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ed4cc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 3589,
     "status": "ok",
     "timestamp": 1702554285866,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "75b6c029"
   },
   "outputs": [],
   "source": [
    "# Splitting the data, seed for reproducibility\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41e4dfd",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1702554285867,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "92a7b7d0"
   },
   "outputs": [],
   "source": [
    "# Defining the augmentations\n",
    "\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "])\n",
    "\n",
    "def augment_images():\n",
    "    x_train_out = []\n",
    "    y_train_out = []\n",
    "\n",
    "    for i in range (len(x_train)):\n",
    "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "        x_train_out.append(ug['image'])\n",
    "        y_train_out.append(ug['mask'])\n",
    "\n",
    "    return np.array(x_train_out), np.array(y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15bc0279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6886,
     "status": "ok",
     "timestamp": 1702554292749,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "1609dd32",
    "outputId": "2a27a292-a3aa-483b-f382-da8afc060e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DUCK-Net\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4c11e8",
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1702554292749,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "2e513d42"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd031cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ef712b9",
    "outputId": "069e3897-d1e8-4bc9-98aa-6e8cbbd7794d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, epoch 0\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 286s 979ms/step - loss: 0.6326 - val_loss: 0.6759\n",
      "Loss Validation: 0.65623724\n",
      "Loss Test: 0.6208231\n",
      "Training, epoch 1\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 198s 989ms/step - loss: 0.5636 - val_loss: 0.6276\n",
      "Loss Validation: 0.61390686\n",
      "Loss Test: 0.57008326\n",
      "Training, epoch 2\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.5410 - val_loss: 0.6128\n",
      "Loss Validation: 0.61209965\n",
      "Loss Test: 0.5703418\n",
      "Training, epoch 3\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.5305 - val_loss: 0.5661\n",
      "Loss Validation: 0.5612775\n",
      "Loss Test: 0.4947381\n",
      "Training, epoch 4\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.5070 - val_loss: 0.5803\n",
      "Loss Validation: 0.57614505\n",
      "Loss Test: 0.49988955\n",
      "Training, epoch 5\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.4879 - val_loss: 0.5064\n",
      "Loss Validation: 0.50044745\n",
      "Loss Test: 0.45137477\n",
      "Training, epoch 6\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 967ms/step - loss: 0.4849 - val_loss: 0.5279\n",
      "Loss Validation: 0.5201505\n",
      "Loss Test: 0.4609741\n",
      "Training, epoch 7\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.4848 - val_loss: 0.4808\n",
      "Loss Validation: 0.46765453\n",
      "Loss Test: 0.42584068\n",
      "Training, epoch 8\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.4746 - val_loss: 0.4668\n",
      "Loss Validation: 0.4581777\n",
      "Loss Test: 0.43838233\n",
      "Training, epoch 9\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.4555 - val_loss: 0.4934\n",
      "Loss Validation: 0.48362768\n",
      "Loss Test: 0.43843818\n",
      "Training, epoch 10\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.4566 - val_loss: 0.4663\n",
      "Loss Validation: 0.45983702\n",
      "Loss Test: 0.4064126\n",
      "Training, epoch 11\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 0.4284 - val_loss: 0.4518\n",
      "Loss Validation: 0.45102286\n",
      "Loss Test: 0.42341214\n",
      "Training, epoch 12\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.4237 - val_loss: 0.4583\n",
      "Loss Validation: 0.45024884\n",
      "Loss Test: 0.3934452\n",
      "Training, epoch 13\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.4013 - val_loss: 0.3894\n",
      "Loss Validation: 0.39209312\n",
      "Loss Test: 0.35237646\n",
      "Training, epoch 14\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 0.3833 - val_loss: 0.3805\n",
      "Loss Validation: 0.375345\n",
      "Loss Test: 0.34467697\n",
      "Training, epoch 15\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3748 - val_loss: 0.3820\n",
      "Loss Validation: 0.3825559\n",
      "Loss Test: 0.29671413\n",
      "Training, epoch 16\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3625 - val_loss: 0.3450\n",
      "Loss Validation: 0.33972865\n",
      "Loss Test: 0.33890152\n",
      "Training, epoch 17\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.3511 - val_loss: 0.3716\n",
      "Loss Validation: 0.3741756\n",
      "Loss Test: 0.3198266\n",
      "Training, epoch 18\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3562 - val_loss: 0.3464\n",
      "Loss Validation: 0.35306627\n",
      "Loss Test: 0.28321362\n",
      "Training, epoch 19\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.3409 - val_loss: 0.3236\n",
      "Loss Validation: 0.334176\n",
      "Loss Test: 0.28350443\n",
      "Training, epoch 20\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 0.3391 - val_loss: 0.3205\n",
      "Loss Validation: 0.31711555\n",
      "Loss Test: 0.31066245\n",
      "Training, epoch 21\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3180 - val_loss: 0.3173\n",
      "Loss Validation: 0.3164662\n",
      "Loss Test: 0.28668934\n",
      "Training, epoch 22\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.3279 - val_loss: 0.3185\n",
      "Loss Validation: 0.31948197\n",
      "Loss Test: 0.2704237\n",
      "Training, epoch 23\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3134 - val_loss: 0.2718\n",
      "Loss Validation: 0.27309006\n",
      "Loss Test: 0.25173932\n",
      "Training, epoch 24\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 194s 968ms/step - loss: 0.3204 - val_loss: 0.3453\n",
      "Loss Validation: 0.3394971\n",
      "Loss Test: 0.28230917\n",
      "Training, epoch 25\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.3177 - val_loss: 0.2757\n",
      "Loss Validation: 0.27298564\n",
      "Loss Test: 0.26791155\n",
      "Training, epoch 26\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 967ms/step - loss: 0.2892 - val_loss: 0.2844\n",
      "Loss Validation: 0.2830122\n",
      "Loss Test: 0.24999177\n",
      "Training, epoch 27\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.3019 - val_loss: 0.3235\n",
      "Loss Validation: 0.32616454\n",
      "Loss Test: 0.26009858\n",
      "Training, epoch 28\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.2978 - val_loss: 0.2622\n",
      "Loss Validation: 0.2668454\n",
      "Loss Test: 0.24970722\n",
      "Training, epoch 29\n",
      "Learning Rate: 0.0001\n",
      "140/200 [====================>.........] - ETA: 55s - loss: 0.2961"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "\n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    image_augmented, mask_augmented = augment_images()\n",
    "\n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "\n",
    "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
    "\n",
    "    prediction_valid = model.predict(x_valid, verbose=0)\n",
    "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
    "\n",
    "    loss_valid = loss_valid.numpy()\n",
    "    print(\"Loss Validation: \" + str(loss_valid))\n",
    "\n",
    "    prediction_test = model.predict(x_test, verbose=0)\n",
    "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
    "    loss_test = loss_test.numpy()\n",
    "    print(\"Loss Test: \" + str(loss_test))\n",
    "\n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
    "\n",
    "    if min_loss_for_saving > loss_valid:\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(\"Saved model with val_loss: \", loss_valid)\n",
    "        model.save(model_path)\n",
    "\n",
    "    del image_augmented\n",
    "    del mask_augmented\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d8c8b",
   "metadata": {
    "id": "2cd52447"
   },
   "outputs": [],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "print(\"Loading the model\")\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
    "\n",
    "prediction_train = model.predict(x_train, batch_size=4)\n",
    "prediction_valid = model.predict(x_valid, batch_size=4)\n",
    "prediction_test = model.predict(x_test, batch_size=4)\n",
    "\n",
    "print(\"Predictions done\")\n",
    "\n",
    "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Dice finished\")\n",
    "\n",
    "\n",
    "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Miou finished\")\n",
    "\n",
    "\n",
    "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
    "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
    "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Precision finished\")\n",
    "\n",
    "\n",
    "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_train > 0.5))\n",
    "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_test > 0.5))\n",
    "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Recall finished\")\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_train > 0.5))\n",
    "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                               np.ndarray.flatten(prediction_test > 0.5))\n",
    "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "\n",
    "print(\"Accuracy finished\")\n",
    "\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
    "print(final_file)\n",
    "\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write(dataset_type + '\\n\\n')\n",
    "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
    "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
    "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
    "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
    "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
    "\n",
    "print('File done')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
